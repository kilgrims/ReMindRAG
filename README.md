<center><h1>ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal</h1></center>


<div style="text-align:center">
  <img src="./assets/workflow.png" style="width:100%;" alt="ReMindRAG Overall Workflow">
</div>

### â€‹â€‹ReMindRAG (Retrieve and Memorize)â€‹â€‹ enhances RAG systems by leveraging LLM-guided knowledge graph traversal for efficient, fine-grained retrieval. 

Unlike traditional methods, it resolves long dependencies and multi-hop reasoning while minimizing computational overhead. By memorizing traversal paths without additional training, ReMindRAG boosts accuracy and reduces retrieval costs for similar queries. Experiments show superior performance in complex tasksâ€”especially multi-hop reasoning and long-range dependenciesâ€”with improved robustness, adaptability, and cost efficiency compared to existing approaches.

## Installation

### Initialize Environment

Use this command to initialize this Repo's environment.
```shell
    conda create -n ReMindRag python==3.13.2
    conda activate ReMindRag
    pip install -r requirements.txt
```

## Quick Start

### Preparation

Replace with your OpenAI API key by filling in the `api_key.json` file:

```json
[
    {
        "base_url": "your_api_key_url",
        "api_key": "your_api_key"
    }
]
```

Download the HuggingFace model [nomic-ai/nomic-embed-text-v2-moe](https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe) and place it in the `./model_cache` directory.

Load your HuggingFace token:

```shell
$env:HF_TOKEN = "hf_YourTokenHere"
```



### Run Example

In the `example` folder, we provide a demo. Use the following command to run this example:
```shell
    cd example
    python example.py
```

## Parameter Configuration
<details>
<summary>Initialization Parameters</summary>

| Parameter                      | Type | Default     | Description |
|--------------------------------|--------|------------------|-------------|
| `edge_weight_coefficient` | Float | 0.1              | Adjusts system's reliance on edge embedding for strong links (range 0.1-0.2) |
| `strong_connection_threshold` | Float | 0.55           | Below 0.775 theoretically; practical range 0.5-0.75 balances retrieval cost and memory capacity |
| `synonym_threshold` | Float | 0.7             | Merges entities when embedding similarity exceeds this value |
| `database_description` | Str | None           | A brief one-sentence description of your data |  
| `save_dir` | Str | None           | Your data storage path |  
| `logger_level` | Int | None           | Logger level based on logging (Level 5 is "Trace," the lowest level we set) |  
| `log_path` | Str | None           | Your log storage path |

</details>

<details>
<summary>Query Parameters</summary>

| Parameter                      | Type | Default    | Description |
|--------------------------------|--------|------------------|-------------|
| `max_jumps`          | Int | 10               | Controls nodes expanded during subgraph queries |
| `max_split_question_num` | Int | 1               | Maximum sub-questions from semantic decomposition (Set 1 to skip question split) |
| `search_key_nums`    | Int | 2                | Number of seed nodes in query initialization |
| `system_prompt`    | Str | None              | System prompt (if not set, our default system prompt will be used) |  
| `force_do_rag`    | Bool | False                | If set to False, the system will automatically decide whether to perform the RAG process |  
| `do_update`    | Bool | True                | If set to False, the memory function will be disabled for this query |

</details>


## Use Your Own Core Components

In **ReMindRAG**, apart from the core modules, other components are highly customizable. We provide a minimal example in `example/example.py`. You can refer to the following methods to use your own custom components.


<details>
<summary>Use Your Own Language Model</summary>

For all language models that use the OpenAI-compatible API format, please use the following code:

```python
from ReMindRag.llms import OpenaiAgent
agent = OpenaiAgent("your_api_key_url", "your_api_key", "your model name")
```

If you want to use another method to call your LLM, please subclass the **AgentBase** class in `ReMindRag/llms/base.py`.

</details>


<details>
<summary>Use Your Own Embedding Model</summary>

For all embedding models using the OpenAI-compatible API format, use the following code:

```python
from ReMindRag.embeddings import OpenaiEmbedding
embedding = OpenaiEmbedding("your_api_key_url", "your_api_key", "your model name")
```

For all embedding models using HuggingFace's SentenceTransformer, use the following code:

```python
from ReMindRag.embeddings import HgEmbedding
embedding = HgEmbedding("your model name", "your model cache dir")
```

If you want to use another embedding model access method, please subclass the **EmbeddingBase** class in `ReMindRag/embeddings/base.py`.

</details>


<details>
<summary>Use Your Own Chunk Function</summary>

For the most basic token-based chunking, use the following code:

```python
from ReMindRag.chunking import NaiveChunker
chunker = NaiveChunker("your tokenizer name", "your tokenizer cache dir", max_token_length=your_chunk_size)
```

If you want to implement a different chunking strategy, please subclass the **ChunkerBase** class in `ReMindRag/chunking/base.py`.

</details>




## Evaluation
<details>
<summary>Code & Steps</summary>

**Step 1**: Download the LooGLE dataset and our modified dataset, then place them in `eval/database`.

**Step 2**: Download the HuggingFace model [nomic-ai/nomic-embed-text-v2-moe](https://huggingface.co/nomic-ai/nomic-embed-text-v2-moe) and place it in the `./model_cache` directory.

**Step 3**: Load your HuggingFace token:

```shell
$env:HF_TOKEN = "hf_YourTokenHere"
```

**Step 4**: Run the following commands to test ReMindRAG.

```shell
cd eval
python start_LooGLE.py --test_name "eval-long" --data_type "longdep_qa"
python start_LooGLE.py --test_name "eval-simple" --data_type "shortdep_qa"
python start_Hotpot.py --test_name "eval-multihop"
```

For more parameters, use the "--help" command.

</details>


## Code Structure
<details>
<summary>Code Structure</summary>

```
ğŸ“‚ ReMindRag
â”œâ”€â”€  ğŸ“‚ ReMindRag
â”‚   â”œâ”€â”€  ğŸ rag_main.py         # Main entry file for ReMindRag
â”‚   â”œâ”€â”€  ğŸ§© chunking
â”‚   â”‚   â”œâ”€â”€  ğŸ base.py         # All text chunking methods must inherit from the class in this file
â”‚   â”‚   â””â”€â”€  ...... 
â”‚   â”œâ”€â”€  ğŸ¤– llms
â”‚   â”‚   â”œâ”€â”€  ğŸ base.py         # All large language model interfaces must inherit from the class in this file
â”‚   â”‚   â””â”€â”€  ...... 
â”‚   â”œâ”€â”€  ğŸ“ embeddings
â”‚   â”‚   â”œâ”€â”€  ğŸ base.py         # All embedding methods must inherit from the class in this file
â”‚   â”‚   â””â”€â”€  ...... 
â”‚   â”œâ”€â”€  ğŸ—ƒï¸ database
â”‚   â”‚   â”œâ”€â”€  ğŸ chromaDB.py     # Database operations based on ChromaDB
â”‚   â”‚   â”œâ”€â”€  ğŸ data_extract.py # Code for extracting named entities and relationships
â”‚   â”‚   â””â”€â”€  ğŸ prompts.py      # Prompts used for extracting named entities and relationships
â”‚   â”œâ”€â”€  ğŸ› ï¸ generator
â”‚   â”‚   â”œâ”€â”€  ğŸ preprocess.py   # Query preprocessing related code
â”‚   â”‚   â”œâ”€â”€  ğŸ pathfinder.py   # Core code for LLM-Guided Traversal
â”‚   â”‚   â””â”€â”€  ğŸ prompts.py      # Prompts used in LLM-Guided Traversal
â”‚   â”œâ”€â”€  ğŸ•¸ï¸ kg                  # Knowledge graph visualization related code
â”‚   â”œâ”€â”€  ğŸ§° utils               # Utility components used in the project
â”‚   â””â”€â”€  ğŸ–¥ï¸ webui
â”‚       â”œâ”€â”€  ğŸ“‚ templates       # Webpage templates used by the web UI
â”‚       â””â”€â”€  ğŸ webui.py        # Main entry code for the web UI
â”œâ”€â”€  ğŸ“œ api_key.json            # JSON file for storing API keys (optional)
â”œâ”€â”€  ğŸ“‚ eval                    # Evaluation code for ReMindRag
â”œâ”€â”€  ğŸ“‚ example                 # Example code for ReMindRag
â””â”€â”€  ......

```

</details>
